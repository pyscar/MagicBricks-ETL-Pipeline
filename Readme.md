# MagicBricks Property Data Scraper & ETL Pipeline

An end-to-end **ETL (Extract â†’ Transform â†’ Load)** web scraping application that collects real-estate listings from **MagicBricks**, cleans and normalizes the data, and provides downloadable CSV outputs through an interactive **Streamlit** interface.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)[![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-bs4-green)](https://www.crummy.com/software/BeautifulSoup/)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-yellow)](https://pandas.pydata.org/)[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://streamlit.io/)
[![Web%20Scraping](https://img.shields.io/badge/Web%20Scraping-ETL-purple)](#)[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success)](#)


---

## ğŸš€ Live Demo
ğŸ”— **Streamlit App:**  
ğŸ‘‰ https://<your-streamlit-app-link>.streamlit.app

*(Replace with your actual deployed link)*

---

## ğŸ§  Project Overview

This project automates the process of collecting and preparing real-estate data for analysis.

### âœ”ï¸ What it does
- Scrapes property listings from MagicBricks
- Handles pagination automatically
- Cleans inconsistent location formats (city & locality)
- Normalizes prices, areas, and property attributes
- Provides raw and cleaned CSV downloads
- Visualizes an ETL pipeline inside a Streamlit app

---

## ğŸ” ETL Pipeline

ğŸŒ **Extract** â†’ ğŸ§¹ **Transform** â†’ ğŸ“¦ **Load**

- **Extract:** Web scraping using BeautifulSoup  
- **Transform:** Data cleaning, validation, normalization  
- **Load:** Structured CSV files ready for analysis  

---

### Tools & Libraries
- **Python**
- **BeautifulSoup (bs4)**
- **Requests**
- **Pandas**
- **Streamlit**
- **LXML**

---

## ğŸ“‚ Project Structure

```

magicbricks-etl-pipeline/
â”‚
â”œâ”€â”€ app.py                     # Streamlit application
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py            # Marks scraper as a Python package
â”‚   â”œâ”€â”€ scraper.py             # Scraping controller
â”‚   â”œâ”€â”€ fetcher.py             # HTTP requests
â”‚   â”œâ”€â”€ parser.py              # HTML parsing
â”‚   â”œâ”€â”€ paginator.py           # Pagination logic
â”‚   â””â”€â”€ config.py              # Headers & constants
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py            # Utility package
â”‚   â””â”€â”€ data_cleaner.py        # Data cleaning & transformation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw CSV samples
â”‚   â””â”€â”€ processed/             # Cleaned CSV samples
â”‚
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml            # Streamlit theme configuration


````

---

## ğŸ–¥ï¸ How to Run Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/magicbricks-etl-pipeline.git
cd magicbricks-etl-pipeline
````

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate      # Mac/Linux
.venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run Streamlit app

```bash
streamlit run app.py
```

---

## ğŸ“¥ Outputs

### Raw Data

* Original scraped data
* Minimal processing
* Useful for debugging or re-processing

### Cleaned Data

* Standardized prices (INR / Lakh)
* Clean locality & city extraction
* Normalized property attributes
* Analysis-ready format

---

## ğŸ§  Key Challenges Solved

* Mixed location formats (comma & space separated)
* Missing project names
* City vs locality misclassification (e.g., *New Delhi*)
* Pagination handling
* Safe scraping with headers & timeouts

---

## ğŸ“Œ Future Enhancements

* Add price & BHK filters
* Interactive charts (price distribution)
* Database storage (PostgreSQL / SQLite)
* Scheduled scraping
* API layer

---
## âš ï¸ Note on Web Scraping

MagicBricks actively blocks scraping from cloud environments such as:

- Streamlit Cloud
- GitHub Codespaces
- AWS / GCP / Azure servers

### âœ… Expected behavior
- Scraper works when run **locally**
- Streamlit cloud app demonstrates:
  - ETL pipeline
  - Data cleaning logic
  - CSV previews
  - UI & workflow

## âš ï¸ Disclaimer
This project is intended for **educational and portfolio purposes**.
Please respect MagicBricksâ€™ terms of service when scraping data.

---

## ğŸ‘¤ Author

**Oscar Kiamba**
Computer Science (AI & ML) | Data Engineering & Analytics Enthusiast

ğŸ“« Connect with me on GitHub & LinkedIn

---








