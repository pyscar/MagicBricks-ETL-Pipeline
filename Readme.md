# MagicBricks Property Data Scraper & ETL Pipeline

An end-to-end **ETL (Extract â†’ Transform â†’ Load)** web scraping application that collects real-estate listings from **MagicBricks**, cleans and normalizes the data, and provides downloadable CSV outputs through an interactive **Streamlit** interface.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)[![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-bs4-green)](https://www.crummy.com/software/BeautifulSoup/)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-yellow)](https://pandas.pydata.org/)[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://streamlit.io/)
[![Web%20Scraping](https://img.shields.io/badge/Web%20Scraping-ETL-purple)](#)[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success)](#)


---

## ðŸš€ Live Demo
ðŸ”— **Streamlit App:**  
ðŸ‘‰ https://magicbricks-etl-pipeline-zprhaau8atehq76sgrfpyy.streamlit.app/

---

## ðŸ§  Project Overview

This project automates the process of collecting and preparing real-estate data for analysis.

### âœ”ï¸ What it does
- Scrapes property listings from MagicBricks
- Handles pagination automatically
- Cleans inconsistent location formats (city & locality)
- Normalizes prices, areas, and property attributes
- Provides raw and cleaned CSV downloads
- Visualizes an ETL pipeline inside a Streamlit app

---

## ðŸ” ETL Pipeline

ðŸŒ **Extract** â†’ ðŸ§¹ **Transform** â†’ ðŸ“¦ **Load**

- **Extract:** Web scraping using BeautifulSoup  
- **Transform:** Data cleaning, validation, normalization  
- **Load:** Structured CSV files ready for analysis  

---

### Tools & Libraries
- **Python**
- **BeautifulSoup (bs4)**
- **Requests**
- **Pandas**
- **Streamlit**
- **LXML**

---

## ðŸ“‚ Project Structure

```

magicbricks-etl-pipeline/
â”‚
â”œâ”€â”€ app.py                     # Streamlit application
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py            # Marks scraper as a Python package
â”‚   â”œâ”€â”€ scraper.py             # Scraping controller
â”‚   â”œâ”€â”€ fetcher.py             # HTTP requests
â”‚   â”œâ”€â”€ parser.py              # HTML parsing
â”‚   â”œâ”€â”€ paginator.py           # Pagination logic
â”‚   â””â”€â”€ config.py              # Headers & constants
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py            # Utility package
â”‚   â””â”€â”€ data_cleaner.py        # Data cleaning & transformation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw CSV samples
â”‚   â””â”€â”€ processed/             # Cleaned CSV samples
â”‚
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml            # Streamlit theme configuration


````

---

### 1ï¸âƒ£ Clone the repository
```bash
git clone [https://github.com/pyscar/MagicBricks-ETL-Pipeline]
cd magicbricks-etl-pipeline
````

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate      # Mac/Linux
.venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run Streamlit app

```bash
streamlit run app.py
```

---

## ðŸ“¥ Outputs

### Raw Data

* Original scraped data
* Minimal processing
* Useful for debugging or re-processing

### Cleaned Data

* Standardized prices (INR / Lakh)
* Clean locality & city extraction
* Normalized property attributes
* Analysis-ready format

---

## ðŸ§  Key Challenges Solved

* Mixed location formats (comma & space separated)
* Missing project names
* City vs locality misclassification (e.g., *New Delhi*)
* Pagination handling
* Safe scraping with headers & timeouts

---

## ðŸ“Œ Future Enhancements

* Add price & BHK filters
* Interactive charts (price distribution)
* Database storage (PostgreSQL / SQLite)
* Scheduled scraping
* API layer

---
## âš ï¸ Note on Web Scraping

MagicBricks actively blocks scraping from cloud environments such as:

- Streamlit Cloud
- GitHub Codespaces
- AWS / GCP / Azure servers

### âœ… Expected behavior
- Scraper works when run **locally** and through the web
- Streamlit cloud app demonstrates:
  - ETL pipeline
  - Data cleaning logic
  - CSV previews
  - UI & workflow
---
### âš ï¸ Demo Mode ðŸ§ª

* In some cloud environments, live scraping from MagicBricks may be blocked (HTTP 403).
 When this happens, the app automatically switches to demo mode using sample Mumbai data:

  - ðŸ“„ sample_mumbai_raw_data.csv
 â†’ Preview raw data

  - ðŸ§¹ sample_mumbai_cleaned_data.csv
 â†’ Preview cleaned data

* This ensures you can still explore the ETL pipeline and test all functionality without live data.

## âš ï¸ Disclaimer
This project is intended for **educational and portfolio purposes**.
Please respect MagicBricksâ€™ terms of service when scraping data.

---

## ðŸ‘¤ Author

**Oscar Kiamba**
Computer Science (AI & ML) | Data science & Ml Enthusiast

ðŸ“« Connect with me on GitHub & LinkedIn
ðŸ‘‰ https://github.com/pyscar
ðŸ‘‰ https://www.linkedin.com/in/oscar-kiamba/

## Complete video link for the project
* video link -> https://www.youtube.com/watch?v=VzqiQuwK2C0
















